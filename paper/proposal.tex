\documentclass[10pt,journal]{IEEEtran}
\usepackage{amsfonts}
\usepackage{pgfplots} % to draw axis
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{letterpaper}


\title{Create Compelling Music Using RNN\\
{\Large Fall 2016 SNLP Project Proposal}}

\author{
\IEEEauthorblockN{Qi Feng\IEEEauthorrefmark{1},
Zhiyu Feng\IEEEauthorrefmark{2},
Wenliang Zhao\IEEEauthorrefmark{3}
}

\IEEEauthorblockA{Department of Computer Science,
New York University\\
New York, NY\\
Email: \IEEEauthorrefmark{1}qf264@nyu.edu,
\IEEEauthorrefmark{2}zf499@nyu.edu,
\IEEEauthorrefmark{3}wz927@nyu.edu}}

\begin{document}
\maketitle
\section{Introduction}
In this project, we will introduce a music composition model that utilizes the language modeling techniques.
The music composition problem is closely related to language modeling that studies the connections among tokens in a sequence.
In classical music, a piece of music exists in the form of a composition in musical notation.
Many deep learning techniques including character LSTM have been used to solve this music composition problem.

In their NLP course project\cite{goodmancs}, Goodman et al introduced a canonical encoding for multi-instrument MIDI songs into natural language.
Rock music model is then trained by utilizing character LSTM variants.
As Douglas et al reported\cite{eck2002first}, music composed by recurrent neural networks (RNNs) generally suffers from a lack of global structure,
since RNNs might not keep track of temporally distant events that indicate global music structure.
They argued that LSTM is a good mechanism to learn a music model and compose music with a relatively good global structure.

In our project, we will implement an LSTM model using TensorFlow\cite{tensorflow2015-whitepaper}.
We will train our model on different categories of music, including classical, rock, pop etc.
The baseline of our work is the other course project we've found on Google Scholar\cite{goodmancs}.
However, even though the task is a bit similar, our work will examine multiple genre of music instead of just rock music.
The evaluation metric is not decided yet, but there might be two approaches.
\begin{enumerate}
  \item
  We give a given number of consecutive sections to our model, and use it to generate the next section.
  The result is then compared with the original section in our corpus.
  \item
  We give our model the first given number of sections of a song, and use the model to generate the entire song.
  In this case, our model should also learn a stopping pattern and prevent an endless generation.
  The result is compared with the original song in our corpus.
\end{enumerate}
\section{Previous Work}
Plenty work have been done in this field.
As a natural thought, people first tried to apply traditional NLP model to this topic.
Darrell\cite{conklin2003music} in 2003 build an n-gram model for this job.
However, most of recent research work are focused on using neural network.
Doug Eck\cite{eck2002first}, mainly uses RNN with LSTMs to do to do blues improvisation. 
In his model, all chosen sequences have the same set of chords.
And the model will output the probability for each note that being played at each time step.
The result of this model is promising in that it can learns temporal structure of the training data.
But this is pretty restricted to what it can output.
Another problem of their model is that it cannot re-articulate held notes,
because their model doesn't differentiate playing a note and holding it.
In 2012, Nicolas etc\cite{boulanger2012modeling} uses a two parts network.
The first one is an RNN designed to capture time dependency by producing a set of outputs which will later be used as the
parameters for the second part.
The second one is a restricted Boltzmann machine that used to model the conditional distribution of notes.
This model can produces some quite fluent music,
but does not model the time dependency quite well. Most of music generated by it are no more than a couple of chords.


\section{Methodology and Evaluation}
As is discussed in the introduction, our problem fits into a character level recurrent neural network (RNN) model.
In the basic model, inputs are the notes at each time step, and for each step, there is a multi-layer network representing the hidden states.
Hidden states are identical at each step which combines inputs and propagate to itself – the recurrent state.
By doing this, the model combines information on a long sequence and builds a long term memory.
However, the derivatives through a long sequence may have the problem of vanishing.
To solve this problem, the long short term memory (LSTM) and gated recurrent unit (GRU) are typical solutions.
In the music composition RNN model, there are 3 dimensions:
1) the time axis representing the sequence of notes being played;
2) the note dimension – there may be multiple channels in a music and multiple notes may be played together at one time step;
3) the hidden state dimension similar to typical RNN.
With this structure, to maintain time invariant – the structure of network keeps the same at each time – is trivial.
But the challenge is how to maintain the note invariant – to make the model being freely transposed the music.
Johnson suggested a model called Biaxial-RNN which is similar to convolutional neural network which considers adjacent notes in the note dimension and applies 1D kernel when propagating the notes to the hidden state.
This method indeed is reasonable, and we will investigate this model into details in this study.

We choose a character based RNN using LSTM as our baseline model which is also the common method using deep learning for music composition.
A couple of different model including the Biaxial-RNN will be compared with the baseline.
The major metrics used to evaluate models is the predictability – given a sequence of music and predict the next time step.
We also consider the ability of a model that compose better music. This evaluation is more subjective but more interesting.
We found most previous studies focused on classic music which likely have more rule in composition.
In this study we will trying to apply the models onto classical, rock and pop music to see if we can compose some reasonable sounds.
\bibliographystyle{IEEEtran}
\bibliography{sections/bib}
\end{document}
